\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brown}
\citation{brown}
\citation{ransac}
\citation{snavely}
\citation{sba}
\citation{furukawa}
\citation{furukawa}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}\hskip -1em.\nobreakspace  {}Feature Matching}{1}{subsection.1.1}}
\@writefile{brf}{\backcite{brown}{{1}{1.1}{subsection.1.1}}}
\@writefile{brf}{\backcite{brown}{{1}{1.1}{subsection.1.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}\hskip -1em.\nobreakspace  {}Camera Calibration}{1}{subsection.1.2}}
\@writefile{brf}{\backcite{ransac}{{1}{1.2}{subsection.1.2}}}
\@writefile{brf}{\backcite{snavely, sba}{{1}{1.2}{subsection.1.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}\hskip -1em.\nobreakspace  {}Dense Reconstruction}{1}{subsection.1.3}}
\@writefile{brf}{\backcite{furukawa}{{1}{1.3}{subsection.1.3}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Point Based Approaches}{1}{subsubsection.1.3.1}}
\citation{spacecarving}
\citation{sgm}
\citation{taxonomy}
\citation{fuse}
\citation{middlebury}
\citation{middlebury}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{brown}{1}
\bibcite{ransac}{2}
\bibcite{fuse}{3}
\bibcite{furukawa}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Sample input image (one of 312)}}{2}{figure.2}}
\newlabel{fig:long}{{2}{2}{Sample input image (one of 312)}{figure.2}{}}
\newlabel{fig:onecol}{{2}{2}{Sample input image (one of 312)}{figure.2}{}}
\@writefile{brf}{\backcite{furukawa}{{2}{1.3.1}{subsubsection.1.3.1}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Volumetric Approaches}{2}{subsubsection.1.3.2}}
\@writefile{brf}{\backcite{spacecarving}{{2}{1.3.2}{subsubsection.1.3.2}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Stereo Depth Approaches}{2}{subsubsection.1.3.3}}
\@writefile{brf}{\backcite{sgm,taxonomy}{{2}{1.3.3}{subsubsection.1.3.3}}}
\@writefile{brf}{\backcite{fuse}{{2}{1.3.3}{subsubsection.1.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}\hskip -1em.\nobreakspace  {}Surface Reconstruction}{2}{subsection.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Our Approach}{2}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sparse Reconstruction}}{2}{figure.3}}
\newlabel{fig:long}{{3}{2}{Sparse Reconstruction}{figure.3}{}}
\newlabel{fig:onecol}{{3}{2}{Sparse Reconstruction}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Dense Patch reconstruction}}{2}{figure.4}}
\newlabel{fig:long}{{4}{2}{Dense Patch reconstruction}{figure.4}{}}
\newlabel{fig:onecol}{{4}{2}{Dense Patch reconstruction}{figure.4}{}}
\bibcite{sgm}{5}
\bibcite{spacecarving}{6}
\bibcite{sba}{7}
\bibcite{taxonomy}{8}
\bibcite{middlebury}{9}
\bibcite{snavely}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Proposed Multi-View pre-processing and visualization pipeline, run on the Temple dataset.\cite  {middlebury} After extracting features and camera calibration parameters from imported source images, point cloud reconstructions can be created from robust computer vision packages such as Bundler and CMVS/PMVS. As a result, 3D editing software tools such as MeshLab and Blender can assist in further configuration and production of detailed meshes/textures. These can be visualized in the Unity3D game engine, which supports Oculus Rift OVR integration.}}{3}{figure.1}}
\@writefile{brf}{\backcite{middlebury}{{3}{1}{figure.1}}}
\newlabel{fig:short}{{1}{3}{Proposed Multi-View pre-processing and visualization pipeline, run on the Temple dataset.\cite {middlebury} After extracting features and camera calibration parameters from imported source images, point cloud reconstructions can be created from robust computer vision packages such as Bundler and CMVS/PMVS. As a result, 3D editing software tools such as MeshLab and Blender can assist in further configuration and production of detailed meshes/textures. These can be visualized in the Unity3D game engine, which supports Oculus Rift OVR integration}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Poisson Surface Reconstruction to fit mesh to patches}}{3}{figure.5}}
\newlabel{fig:long}{{5}{3}{Poisson Surface Reconstruction to fit mesh to patches}{figure.5}{}}
\newlabel{fig:onecol}{{5}{3}{Poisson Surface Reconstruction to fit mesh to patches}{figure.5}{}}
