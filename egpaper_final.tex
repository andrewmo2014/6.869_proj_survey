\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{3D Reconstruction from Multi-View Stereo:\\ From Implementation to Oculus Virtual Reality}

\author{Andrew Moran\\
  MIT, \textit{Class of 2014}\\
  {\tt\small andrewmo@mit.edu}
\and
Ben Eysenbach\\
MIT, \textit{Class of 2017}\\
{\tt\small bce@mit.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   We propose a novel approach to visualizing Multi-View Stereo reconstruction pipelines. We demonstrate how an Oculus Rift can be used to visualize the reconstruction process, allowing researchers to better understand how existing reconstruction algorithms work and where they fail.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Multiview stereo pipelines are complex. Usually east step is visualized in 2D. Sometime the output is viewed in 3D. Regardless, it is difficult to see the effect of small changes to the pipeline if you can only look at the final reconstructed scene. When the pipeline fails entirely, being able to visualize it step by step allows users to figure out which step is failing.

\section{Related Work}
Phototourism

\section{Visualization}
We give methods for visualizing each step in the reconstruction pipeline: camera pose estimation, dense reconstruction, and surface reconstruction. 

Generally, these visualization techniques are helpful for debugging, when ground truth data is unknown, and for choosing `magic parameters' for algorithms.

\subsection{Camera Pose Estimation}

\subsubsection{Feature Matching}
The first step in most Multi-View Stereo pipelines is finding correspondence points between images. Once we know these points, we can determine the relative position of the cameras which took the images.

First, we select features of interest in each image. There exist numerous algorithms for selecting these points (SIFT, SURF). The general idea behind most of them is to find features which are unique enough such that finding a similar feature in another image indicates with high probability that the two features corresponds to the same object in the scene.\cite{brown}

Once features have been extracted from each image, pairwise matches must be found. Matches will not exist for all features, so some criteria must be specified for when to accept a match. One such criteria is to accept a match if the two features are each the best match for the other. Another approach is to match one feature to another feature if the best alternative match is a much worse match than the best match.\cite{brown}

Feature matching doesn't rely on the 3D geometry of the scene, so it doesn't make sense to visualize this step in 3D. Hence, we recommend using existing 2D visualization tools, comparing images side-by-side, to verify the output of feature matching algorithms and tune magic parameters

\subsubsection{Bundle Adjustment}

Now that we have correspondence points, we want to compute a homography relating one image to another. If we only wanted to find the orientation of one camera relative to another, we could use RANSAC to fit a homography (with the Discrete Linear Transform).\cite{ransac} However, if we only found the optimal pairwise relative positions, they probably would not be globally consistent.

Instead, we want to find the \emph{global} optimal camera positions. One approach, Bundle Adjustment, incrementally alters the positions of the cameras and feature points to minimize \emph{reprojective error}. This process can be seen as minimizing a series of nonlinear equations; the Levenberg Marquardt Algorithm for nonlinear least-squares is commonly used as a subroutine. At the end of this process, we have a calibration matrix for each camera, relating the pose of each camera to a global coordinate system, as well as a small set of feature points and their positions in the global coordinate system.\cite{snavely, sba}

% write out the math here?

The output of this process can be difficult to evaluate quantitatively. We can look at the reprojective error of the optimal locations, but that can be difficult to comprehend because it depends on the type of input (image resolution, image scale) as well as how feature matching was performed. If ground truth is known, output of Bundle Adjustment can be compared against it. However, given how few datasets are equipped with camera poses, only verifying your algorithm on one of these risks `overfitting.'

Viewing the camera poses and feature points in 3D solves many of these problems. It is easy for humans to figure out how images were taken with respect to one another. One reason is that humans know prior information on how likely certain types of scenes are. If they are put virtual 3D environment displaying camera poses and feature points, it is easy to check that the orientations and locations are consistent with the input images. 

\subsection{Dense Reconstruction}

We can now reconstruct the scene from calibrated cameras. We want to find eventually output a scene which is \emph{photo consistent}. Common approaches to this problem include: (1) building up a scene from points whose locations are found by triangulating between images; (2) starting with a volume which encloses the region of interest, and removing \emph{voxels} which are not photoconsistent; and (3) generating stereo depth maps for pairs of images, and then fusing them together.\cite{furukawa}

Similar to with Bundle Adjustment, it can be difficult to evaluate the output of dense reconstruction without a known ground truth. Human observers often know the 3D geometry of the object of interest, but are unable to translate their mental model of the object into a quantitative model to evaluate their algorithm against. Nonetheless, if they are able to see the resulting point cloud in 3D, it is easy for hem to verify that the dense reconstruction algorithm performed as desired. If not, they can figure out how they know the point cloud is incorrect, and add that criteria to their algorithm.

\subsection{Surface Reconstruction}
Many multi-view stereo algorithms output a set of points which lie on the surface of objects in the scene. These points can be analyzed at this stage, but it is often preferable to reconstruct the 3D surface(s) from which the points were sampled. This sort of problem also arises when analyzing LIDAR scans.

This can be viewed as an optimization problem in which the overall goal is to minimize a global energy function. This function often enforces a smoothness constraint and penalizes the distance between points and the mesh.\cite{mesh_opt}  Two common approaches to solve this problem include (1) interpolating the points with computational geometry and (2) constructing implicit functions to fit the data.

We used out visualization technique to compare two surface reconstruction algorithms on the same input camera calibration. The two methods were Hoppe 92 and Poisson Surface. We choose these because XXX. We hoped to see XXX. In fact, when we finished, that was exactly what we saw.

\subsubsection{Normal Estimation}

Many surface reconstruction methods require that each point has an estimated normal. This presents a Chicken-and-Egg scenario: to construct a surface you need normals; however, to find the normals, you to know the underlying surface.

Nonetheless, we can estimate the surface normal at a point by looking at nearby points. These points form a probability distribution over point locations. Level surfaces of this distribution (e.g. 90\% of points fall within this surface) form ellipsoids. Remarkable, the axes of these ellipsoids are the eigenvectors of the covariance matrix, scaled by their eigenvalue. Then, we can approximate the surface normal by taking the eigenvector with the smallest eigenvalue.

Eigenvectors are invariant to scale, so we need to orient the normals such that they all point from interior to exterior. We do this by fixing the orientation of a known point (at an extrema), and propagating orientation to neighboring points. Specifically, we find the neighboring point which has the normal most parallel to an oriented point, and then flip this point's normal if necessary. This method is analogous to creating a minimum spanning tree over the points, where the distance between two points, $p_1, p_2$ is defined as: $d(p_1, p_2) = 1-p_1.normal \cdot p_2.normal$.

Similar to previous steps in the pipeline, it is easier to evaluate the orientation of the normals when visualized in 3D. It is easy for a human to verify that the normals look correct, but it can be difficult to explain exactly why they are correct.

%ADD ANOTHER TWO SENTENCES OF EXPLANATION

\subsubsection{Octree Representation}

\subsubsection{Signed Distance Methods}

\cite{surface_review, hoppe1992surface}

\subsubsection{Poisson Methods}

\cite{surface_review, poisson}

\section{Experiments}
% do we even need this section?

\section{Discussion}

{\small
  \bibliographystyle{ieee}
  \bibliography{egbib}
}

\end{document}
