\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{3D Reconstruction from Multi-View Stereo:\\ From Implementation to Oculus Virtual Reality}

\author{Andrew Moran\\
  MIT, \textit{Class of 2014}\\
  {\tt\small andrewmo@mit.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Ben Eysenbach\\
MIT\\
{\tt\small bce@mit.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Multi-View Stereo reconstructs a 3D model from images. Each image is a projection of a 3D model onto the camera plane ($\mathbb{R}^3 \rightarrow \mathbb{R}^2$), which inherently results in a loss of information. With enough images taken from a variety of perspectives, an reasonable model of the original scene can be reconstructed. These reconstructions usually begin by determining where each image was taken from. Once the cameras are calibrated, a dense, colored point cloud can be generated. A mesh can be fit over the point cloud to represent structures in the original scene. This entire process can be visualized through an Oculus Rift.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Most current approaches to Multi-View Stereo can be broken down into three steps: feature matching, camera calibration, and dense reconstruction. In pipelines which result in a point cloud, surface reconstruction is applied as a final step.
%-------------------------------------------------------------------------
\subsection{Feature Matching}

The first step in most Multi-View Stereo pipelines is finding correspondence points between images. Once we know these points, we can determine the relative position of the cameras which took the images.

First, we select features of interest in each image. There exist numerous algorithms for selecting these points (SIFT, SURF). The general idea behind most of them is to find a feature which is unique enough such that finding a similar feature in another images indicates with high probability that the two features corresponds to the same object in the scene.\cite{brown}

Once features have been extracted from each image, pairwise matches must be found. Matches will not exist for all features, so some criteria must be specified for when to accept a match. One such criteria is to match two features if the first is the best match for the second, and the second is the best match for the first. Another approach is to match one feature to another feature if the second best match is a much worse match than the best match.\cite{brown}

\subsection{Camera Calibration}

Now that we have correspondence points, we want to compute a homography relating one image to another. If we only wanted to find the orientation of one camera relative to another, we could use RANSAC to fit a homography (with the Discrete Linear Transform to make it linear).\cite{ransac} However, if we only found the optimal pairwise relative positions, we would not be guaranteed that they would be consistent.

Instead, we want to find the \emph{global} optimal camera positions. This process is known as Sparse Bundle Adjustment, and can been seen as minimizing a series of nonlinear equations. The  Levenberg Marquardt Algorithm for nonlinear least-squares is commonly used as a subroutine. Sparse Bundler Adjustment incrementally alters the positions of the cameras to as to minimize the \emph{reprojective error} of the found correspondence points with respect to the images in which they appear. At the end of this process, we have a calibration matrix for each camera, relating the pose of each camera to a global coordinate system.\cite{snavely, sba}

\subsection{Dense Reconstruction}

We can now reconstruct the scene from calibrated cameras. We want to find eventually output a scene which is \emph{photo consistent}. Common approaches to this problem include: (1) building up a scene from points whose locations are found by triangulating between images; (2) starting with a volume which encloses the region of interest, and removing \emph{voxels} which are not photoconsistent; and (3) generating stereo depth maps for pairs of images, and then fusing them together.\cite{furukawa}

\begin{figure*}[t]
  \begin{center}
    %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{pipeline3.png}
  \end{center}
  \caption{Proposed Multi-View pipeline. (A) Source Images with known camera parameters. (B) Bundler Adjustment that utilizes feature matching to create a sparse point cloud. (C) Dense Recontruction that performs clustering to develop a more compact point cloud. (D) Surface Reconstruction adds a mesh to the points. (E) Applying colored textures on the mesh results in a realistic simulated model.}
  \label{fig:short}
\end{figure*}

\subsubsection{Point Based Approaches}

Once we have calibrated cameras with a sparse reconstruction, we can search along equipolar lines to find more correspondence points. The real-world locations of these points can be found through triangulation. In Patch-based Multi-View Stereo, these points were further expanded to \emph{patches} which included a color and normal vector.\cite{furukawa}

\subsubsection{Volumetric Approaches}

Another approach to dense reconstruction is to start with a volume which encloses the region of interest, and iteratively remove small sections (\emph{voxels}) which are not photoconsistent. Constructing the initial visual hull requires segmenting the input images into foreground and background.\cite{spacecarving}

\subsubsection{Stereo Depth Approaches}

A final approach is to build off research in stereo matching. Here, we generate depth maps for all pairs of images with overlapping fields of views. Methods include SemiGlobal Matching, Graph Cuts, and Dynamic Programming.\cite{sgm,taxonomy} These depth maps can be fused to extract the structure of the scene.\cite{fuse}


\subsection{Surface Reconstruction}
% Mesh fitting
% Poisson Surface reconstruction
% Iterative Snapping [Furukawa]
% Marching Squares/Tetrahedrons
% Voronoi

\section{Our Approach}


\section{Surface Reconstruction}
After producing a dense reconstruction composed of multiple vertices, the next step is to apply a surface or mesh.  A mesh can be modeled as parametric surfaces.  This can be viewed as an optimization problem to minimize the overall \textit{energy} of the mesh, that aims at a close fit and compact final representation.


% Mesh fitting
% Poisson Surface reconstruction
% Iterative Snapping [Furukawa]
% Marching Squares/Tetrahedrons
% Voronoi

\section{Our Approach}
There are various algorithms and tools in the computer vision community designed for Multi-View Stereo.  Our approach consists of utilizing point-based methods to develop a densely reconstructed point cloud.  From there, we will compare various mesh-fitting surface techniques and potentially implement our own, such as Poisson Surface Reconstruction.  We plan to visualize our results from each step.

\subsection{Proposed Pipeline}
Figure 1 summarizes our pipelined approach of displaying a 3D model from 2D images.  After extracting features and camera calibration parameters from imported source images, point cloud reconstructions can be created from Structure-from-Motion (SfM) tools.  We will utilize robust computer vision packages such as Bundler and CMVS/PMVS [0].  As a result, we will attempt to compare surface reconstruction techniques that will be used to fit a detailed mesh to the point cloud.  3D editing software tools such as MeshLab and Blender[0] can assist in further configuration and production of desired meshes/textures.  These can be visualized in the Unity3D game engine, which supports Oculus Rift OVR integration.


\subsection{Visualization}
With a complete 3D reconstruction, the objective is to then use innovative technology to view and interact with the final scene/model. The Unity3D Game Engine is the utlimate tool for video game development, architectual visualizations, and interactive media installations [0].  By simulating a virtualized environment, the user can continue to explore/examine the results.  As an extension, Unity3D has a fully integrated plugin for the Oculus Rift.  This developemnt tool is an augmented reality head-mounted display that creates a stereoscopic 3D view.  The rift in conjunction with the game engine can produce an immersive setting that enhances the user experience.

\subsection{Conclusion/Motivation}
There are many applications to Multi-View Stereo.  They include (but are not limited to) reverse engineering, industrial design, performance analysis and simulations, realistic virtual environments, and medical imaging.  Converting a series of 2D images to a 3D model can be a challenging process.  However, feature matching and dense/surface reconstruction are techniques that produce appealing results.  Visualizing our results using enhanced 3D tools such as Unity3D and Oculus Rift will assist in understanding and recognizing the 3-Dimensional product.




% \begin{figure}[t]
%   \begin{center}
%     % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%     \includegraphics[width=0.8\linewidth]{temple0001.png}
%   \end{center}
%   \caption{Sample input image (one of 312)}
%   \label{fig:long}
%   \label{fig:onecol}
% \end{figure}

% \begin{figure}[t]
%   \begin{center}
%     % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%     \includegraphics[width=0.8\linewidth]{sparse.png}
%   \end{center}
%   \caption{Sparse Reconstruction}
%   \label{fig:long}
%   \label{fig:onecol}
% \end{figure}

% \begin{figure}[t]
% \begin{center}
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% \includegraphics[width=0.8\linewidth]{patches.png}
% \end{center}
% \caption{Dense Patch reconstruction}
% \label{fig:long}
% \label{fig:onecol}
% \end{figure}

% \begin{figure}[t]
%   \begin{center}
%     % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%     \includegraphics[width=0.8\linewidth]{mesh.png}
%   \end{center}
%   \caption{Poisson Surface Reconstruction to fit mesh to patches}
%   \label{fig:long}
%   \label{fig:onecol}
% \end{figure}



% WE NEED TO ADD OUR SOURCES

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
